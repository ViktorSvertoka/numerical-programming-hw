# -*- coding: utf-8 -*-
"""hw_12_svertoka_viktor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wc27rsNIr2S0u_zYc2J0w8YzLqZfI5JY
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.cluster import SpectralClustering, KMeans
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import confusion_matrix, f1_score, roc_curve, auc
import pygad
import time

# 1. Завантаження даних
data = load_breast_cancer()

# Ознаки
X = data.data
# Цільова змінна (0 - злоякісна, 1 - доброякісна)
y = data.target

# 2. Візуалізація попарних точкових діаграм
df = pd.DataFrame(X, columns=data.feature_names)
df['target'] = y

# Візуалізація попарних точкових діаграм
sns.pairplot(df, hue='target')
plt.show()

# 3. Кластеризація методами Спектральної кластеризації, k_means та моделі сумішей Гаусса

# Спектральна кластеризація
spectral = SpectralClustering(n_clusters=2, affinity='nearest_neighbors')
y_spectral = spectral.fit_predict(X)

# K-means кластеризація
kmeans = KMeans(n_clusters=2)
y_kmeans = kmeans.fit_predict(X)

# Моделі сумішей Гаусса
gmm = GaussianMixture(n_components=2)
y_gmm = gmm.fit_predict(X)

# Порівняння результатів кластеризації
fig, ax = plt.subplots(1, 3, figsize=(15, 5))
ax[0].scatter(X[:, 0], X[:, 1], c=y_spectral)
ax[0].set_title("Spectral Clustering")
ax[1].scatter(X[:, 0], X[:, 1], c=y_kmeans)
ax[1].set_title("K-means")
ax[2].scatter(X[:, 0], X[:, 1], c=y_gmm)
ax[2].set_title("Gaussian Mixture")
plt.show()

# 4. Зменшення розмірності даних за допомогою PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# 5. Візуалізація точкової діаграми в новому просторі ознак
plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)
plt.title("PCA: Breast Cancer Data")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()

# 6. Класифікація методом логістичної регресії
log_reg = LogisticRegression(max_iter=10000)
log_reg.fit(X, y)
y_pred = log_reg.predict(X)

# 7. Класифікація з оптимізацією параметрів за допомогою градієнтного спуску
sgd = SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3)
sgd.fit(X, y)
y_pred_sgd = sgd.predict(X)

# 8. Класифікація з оптимізацією за допомогою генетичного алгоритму
def fitness_function(ga_instance, solution, solution_idx):
    log_reg.fit(X, y)
    return log_reg.score(X, y)

# Зворотна функція для затримки після кожної генерації (можна тимчасово прибрати для швидшого виконання)
def on_generation_callback(ga_instance):
    # time.sleep(1)  # Приберіть затримку для тестування швидкості
    pass

ga = pygad.GA(num_generations=20,  # Зменшено до 20 поколінь для тестування
              num_parents_mating=10,
              fitness_func=fitness_function,
              sol_per_pop=20,
              num_genes=X.shape[1],
              on_generation=on_generation_callback)  # Використовуємо зворотній виклик без затримки

ga.run()

# 9. Оцінка якості класифікації
cm = confusion_matrix(y, y_pred)
f1 = f1_score(y, y_pred)

cm_sgd = confusion_matrix(y, y_pred_sgd)
f1_sgd = f1_score(y, y_pred_sgd)

# ROC Curve
fpr_logreg, tpr_logreg, _ = roc_curve(y, log_reg.predict_proba(X)[:, 1])
roc_auc_logreg = auc(fpr_logreg, tpr_logreg)

fpr_sgd, tpr_sgd, _ = roc_curve(y, sgd.predict_proba(X)[:, 1])
roc_auc_sgd = auc(fpr_sgd, tpr_sgd)

# Порівняння ROC-кривих
plt.figure(figsize=(8, 6))
plt.plot(fpr_logreg, tpr_logreg, color='darkorange', lw=2, label='Logistic Regression (AUC = %0.2f)' % roc_auc_logreg)
plt.plot(fpr_sgd, tpr_sgd, color='blue', lw=2, label='SGDClassifier (AUC = %0.2f)' % roc_auc_sgd)
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()

print("Logistic Regression Confusion Matrix:\n", cm)
print("Logistic Regression F1 Score:", f1)
print("SGD Confusion Matrix:\n", cm_sgd)
print("SGD F1 Score:", f1_sgd)

print("""
Висновки:

1. Методи кластеризації та реальний розподіл класів:
- Спектральна кластеризація, K-means та Gaussian Mixture мають різні підходи до поділу даних, але їх ефективність варіюється. Gaussian Mixture, завдяки ймовірнісним моделям, показує найкращі результати серед них.

2. Роль зменшення розмірності за допомогою PCA:
- PCA знижує розмірність, зберігаючи важливу інформацію, що допомагає візуалізувати класи. Це особливо корисно при великих наборах даних, де важливі ознаки можуть бути сховані серед неінформативних.

3. Оптимізація класифікаторів за допомогою SGDClassifier:
- Градієнтний спуск (SGD) показав добрі результати, використовуючи адаптивний підхід до налаштування параметрів та мінімізації помилки класифікації.

4. Оптимізація за допомогою генетичного алгоритму (PyGAD):
- Генетичний алгоритм може знайти глобальні оптимальні параметри для класифікації, хоча він потребує більше часу на обчислення.

5. Оцінка якості класифікації:
- Logistic Regression і SGDClassifier демонструють високі результати. Всі методи класифікації забезпечують високу точність.

Рекомендації:
- Gaussian Mixture є найбільш доцільним для складних задач з невизначеними межами між класами.
- PCA варто застосовувати обережно, оскільки зменшення розмірності може призвести до втрати деякої інформації.
""")

